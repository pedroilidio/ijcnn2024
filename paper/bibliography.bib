@article{bekker2018estimating,
  title = {Estimating the {{Class Prior}} in {{Positive}} and {{Unlabeled Data Through Decision Tree Induction}}},
  author = {Bekker, Jessa and Davis, Jesse},
  date = {2018-04-29},
  year = {2018},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11715}}
%  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11715},
%  urldate = {2023-12-18},
  abstract = {For tasks such as medical diagnosis and knowledge base completion, a classifier may only have access to positive and unlabeled examples, where the unlabeled data consists of both positive and negative examples. One way that enables learning from this type of data is knowing the true class prior. In this paper, we propose a simple yet effective method for estimating the class prior, by estimating the probability that a positive example is selected to be labeled. Our key insight is that subdomains of the data give a lower bound on this probability. This lower bound gets closer to the real probability as the ratio of labeled examples increases. Finding such subsets can naturally be done via top-down decision tree induction. Experiments show that our method makes estimates which are equivalently accurate as those of the state of the art methods, and is an order of magnitude faster.},
  issue = {1},
  langid = {english},
  keywords = {Positive Unlabeled Class Prior},
  annotation = {36 citations (Crossref) [2023-12-18]},
  file = {/home/pd/Zotero/storage/9M3LDM3L/Bekker and Davis - 2018 - Estimating the Class Prior in Positive and Unlabel.pdf}
}

@article{bekker2020learning,
  title = {Learning from Positive and Unlabeled Data: A Survey},
  shorttitle = {Learning from Positive and Unlabeled Data},
  author = {Bekker, Jessa and Davis, Jesse},
  date = {2020-04-01},
  year = {2020},
  journaltitle = {Machine Learning},
  journal = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {109},
  number = {4},
  pages = {719--760},
  issn = {1573-0565},
  doi = {10.1007/s10994-020-05877-5}
}
%  url = {https://doi.org/10.1007/s10994-020-05877-5},
  urldate = {2024-01-24},
  abstract = {Learning from positive and unlabeled data or PU learning is the setting where a learner only has access to positive examples and unlabeled data. The assumption is that the unlabeled data can contain both positive and negative examples. This setting has attracted increasing interest within the machine learning literature as this type of data naturally arises in applications such as medical diagnosis and knowledge base completion. This article provides a survey of the current state of the art in PU learning. It proposes seven key research questions that commonly arise in this field and provides a broad overview of how the field has tried to address them.},
  langid = {english},
  keywords = {68T05,Classification,PU learning,Weakly supervised learning},
  annotation = {215 citations (Crossref) [2024-01-24]},
  file = {/home/pd/Zotero/storage/V4MIVCVY/Bekker and Davis - 2020 - Learning from positive and unlabeled data a surve.pdf}
}

@article{benavoli2016should,
  title = {Should {{We Really Use Post-Hoc Tests Based}} on {{Mean-Ranks}}?},
  author = {Benavoli, Alessio and Corani, Giorgio and Mangili, Francesca},
  date = {2016},
  year = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {5},
  pages = {1--10},
  issn = {1533-7928}
}
  %url = {http://jmlr.org/papers/v17/benavoli16a.html},
  urldate = {2024-01-12},
  abstract = {The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc.. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms  A {$A$}  and  B {$B$}  depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between  A {$A$}  and  B {$B$}  could be declared significant if the pool comprises algorithms  C,D,E {$C$} , {$D$} , {$E$}  and not significant if the pool comprises algorithms  F,G,H {$F$} , {$G$} , {$H$} . To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test.},
  file = {/home/pd/Zotero/storage/ESJNESRI/Benavoli et al. - 2016 - Should We Really Use Post-Hoc Tests Based on Mean-.pdf}
}

@article{breiman2001random,
  title = {Random Forests},
  author = {Breiman, Leo},
  date = {2001},
  year = {2001},
  journaltitle = {Machine learning},
  journal = {Machine learning},
  volume = {45},
  pages = {5--32},
  publisher = {{Springer}}
}


@inproceedings{elkan2008learning,
  title = {Learning Classifiers from Only Positive and Unlabeled Data},
  booktitle = {Proceedings of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  journaltitle = {Proceedings of the 14th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Elkan, Charles and Noto, Keith},
  date = {2008-08-24},
  year = {2008},
  series = {{{KDD}} '08},
  pages = {213--220},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1401890.1401920},
  url = {https://doi.org/10.1145/1401890.1401920},
  urldate = {2024-01-24},
  abstract = {The input to an algorithm that learns a binary classifier normally consists of two sets of examples, where one set consists of positive examples of the concept to be learned, and the other set consists of negative examples. However, it is often the case that the available training data are an incomplete set of positive examples, and a set of unlabeled examples, some of which are positive and some of which are negative. The problem solved in this paper is how to learn a standard binary classifier given a nontraditional training set of this nature. Under the assumption that the labeled examples are selected randomly from the positive examples, we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. We show how to use this result in two different ways to learn a classifier from a nontraditional training set. We then apply these two new methods to solve a real-world problem: identifying protein records that should be included in an incomplete specialized molecular biology database. Our experiments in this domain show that models trained using the new methods perform better than the current state-of-the-art biased SVM method for learning from positive and unlabeled examples.},
  isbn = {978-1-60558-193-4},
  keywords = {bioinformatics,supervised learning,text mining,unlabeled examples},
  annotation = {487 citations (Crossref) [2024-01-24]},
  file = {/home/pd/Zotero/storage/5PT2V2D5/Elkan and Noto - 2008 - Learning classifiers from only positive and unlabe.pdf}
}

@inproceedings{garg2021mixture,
  title = {Mixture {{Proportion Estimation}} and {{PU Learning}}:{{A Modern Approach}}},
  shorttitle = {Mixture {{Proportion Estimation}} and {{PU Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  journaltitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Garg, Saurabh and Wu, Yifan and Smola, Alexander J and Balakrishnan, Sivaraman and Lipton, Zachary},
  year = {2021},
  date = {2021},
  volume = {34},
  pages = {8532--8544},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/47b4f1bfdf6d298682e610ad74b37dca-Abstract.html},
  urldate = {2023-12-31},
  file = {/home/pd/Zotero/storage/KTSAR2CF/Garg et al. - 2021 - Mixture Proportion Estimation and PU LearningA Mo.pdf}
}

@article{geurts2006extremely,
  title = {Extremely Randomized Trees},
  author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  date = {2006},
  year = {2006},
  journaltitle = {Machine learning},
  journal = {Machine learning},
  volume = {63},
  pages = {3--42},
  publisher = {{Springer}},
  file = {/home/pd/Zotero/storage/3SPLNMXA/Geurts et al. - 2006 - Extremely randomized trees.pdf}
}

@article{liu2016classification,
  title = {Classification with {{Noisy Labels}} by {{Importance Reweighting}}},
  author = {Liu, Tongliang and Tao, Dacheng},
  date = {2016-03},
  year = {2016},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {38},
  number = {3},
  pages = {447--461},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2015.2456899},
  url = {https://ieeexplore.ieee.org/abstract/document/7159100?casa_token=rVemoYbQsooAAAAA:c72v9w99z0p3IabvydtzlsHRo9nQS30jHmCyUgkUXyillfSijS8-eryb_X5qGaIWeayEmVzDntL8},
  urldate = {2023-12-31},
  abstract = {In this paper, we study a classification problem in which sample labels are randomly corrupted. In this scenario, there is an unobservable sample with noise-free labels. However, before being observed, the true labels are independently flipped with a probability \textbackslash rho \i n [0,0.5) , and the random label noise can be class-conditional. Here, we address two fundamental problems raised by this scenario. The first is how to best use the abundant surrogate loss functions designed for the traditional classification problem when there is label noise. We prove that any surrogate loss function can be used for classification with noisy labels by using importance reweighting, with consistency assurance that the label noise does not ultimately hinder the search for the optimal classifier of the noise-free sample. The other is the open problem of how to obtain the noise rate {$\rho$} . We show that the rate is upper bounded by the conditional probability P(\textbackslash hatY\textbar X) of the noisy sample. Consequently, the rate can be estimated, because the upper bound can be easily reached in classification problems. Experimental results on synthetic and real datasets confirm the efficiency of our methods.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  annotation = {401 citations (Crossref) [2023-12-31]},
  file = {/home/pd/Zotero/storage/5G6E4E6C/Liu and Tao - 2016 - Classification with Noisy Labels by Importance Rew.pdf;/home/pd/Zotero/storage/UXJ6REXV/7159100.html}
}

@inproceedings{lundberg2017unified,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  journaltitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  date = {2017},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
  urldate = {2024-01-13},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  file = {/home/pd/Zotero/storage/XXFV7FIA/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@article{nakano2022deep,
  title = {Deep Tree-Ensembles for Multi-Output Prediction},
  author = {Nakano, Felipe Kenji and Pliakos, Konstantinos and Vens, Celine},
  date = {2022-01-01},
  year = {2022},
  journaltitle = {Pattern Recognition},
  journal = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {121},
  pages = {108211},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2021.108211}
}
 % url = {https://www.sciencedirect.com/science/article/pii/S0031320321003927},
  urldate = {2023-12-18},
  abstract = {Recently, deep neural networks have expanded the state-of-art in various scientific fields and provided solutions to long standing problems across multiple application domains. Nevertheless, they also suffer from weaknesses since their optimal performance depends on massive amounts of training data and the tuning of an extended number of parameters. As a countermeasure, some deep-forest methods have been recently proposed, as efficient and low-scale solutions. Despite that, these approaches simply employ label classification probabilities as induced features and primarily focus on traditional classification and regression tasks, leaving multi-output prediction under-explored. Moreover, recent work has demonstrated that tree-embeddings are highly representative, especially in structured output prediction. In this direction, we propose a novel deep tree-ensemble (DTE) model, where every layer enriches the original feature set with a representation learning component based on tree-embeddings. In this paper, we specifically focus on two structured output prediction tasks, namely multi-label classification and multi-target regression. We conducted experiments using multiple benchmark datasets and the obtained results confirm that our method provides superior results to state-of-the-art methods in both tasks.},
  keywords = {Deep-forest,Ensemble learning,Multi-label classification,Multi-output prediction,Multi-target regression},
  annotation = {8 citations (Crossref) [2023-12-18]},
  file = {/home/pd/Zotero/storage/7MXAYVQJ/Nakano et al. - 2022 - Deep tree-ensembles for multi-output prediction.pdf;/home/pd/Zotero/storage/UN6B3RWS/S0031320321003927.html}
}

@article{northcuttlearning,
  title = {Learning with {{Confident Examples}}: {{Rank Pruning}} for {{Robust Classification}} with {{Noisy Labels}}},
  author = {Northcutt, Curtis G and Wu, Tailin and Chuang, Isaac L},
  abstract = {P\texttildelow N\texttildelow{} learning is the problem of binary classification when training examples may be mislabeled (flipped) uniformly with noise rate {$\rho$}1 for positive examples and {$\rho$}0 for negative examples. We propose Rank Pruning (RP) to solve P\texttildelow N\texttildelow{} learning and the open problem of estimating the noise rates. Unlike prior solutions, RP is efficient and general, requiring O(T ) for any unrestricted choice of probabilistic classifier with T fitting time. We prove RP achieves consistent noise estimation and equivalent expected risk as learning with uncorrupted labels in ideal conditions, and derive closed-form solutions when conditions are non-ideal. RP achieves state-of-the-art noise estimation and F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the amount of noise. To highlight, RP with a CNN classifier can predict if an MNIST digit is a one or not with only 0.25\% error, and 0.46\% error across all digits, even when 50\% of positive examples are mislabeled and 50\% of observed positive labels are mislabeled negative examples.},
  langid = {english},
  file = {/home/pd/Zotero/storage/CDMQGBM2/Northcutt et al. - Learning with Conﬁdent Examples Rank Pruning for .pdf}
}

@inproceedings{perini2020class,
  title = {Class {{Prior Estimation}} in {{Active Positive}} and {{Unlabeled Learning}}},
  author = {Perini, Lorenzo and Vercruyssen, Vincent and Davis, Jesse},
  date = {2020-07-01},
  year = {2020},
  pages = {2887--2893},
  doi = {10.24963/ijcai.2020/399},
  abstract = {Estimating the proportion of positive examples (i.e., the class prior) from positive and unlabeled (PU) data is an important task that facilitates learning a classifier from such data. In this paper, we explore how to tackle this problem when the observed labels were acquired via active learning. This introduces the challenge that the observed labels were not selected completely at random, which is the primary assumption underpinning existing approaches to estimating the class prior from PU data. We analyze this new setting and design an algorithm that is able to estimate the class prior for a given active learning strategy. Empirically, we show that our approach accurately recovers the true class prior on a benchmark of anomaly detection datasets and that it does so more accurately than existing methods.},
  annotation = {2 citations (Crossref) [2023-12-31]},
  file = {/home/pd/Zotero/storage/QBJEWNGM/Perini et al. - 2020 - Class Prior Estimation in Active Positive and Unla.pdf}
}

@article{plessis2017classprior,
  title = {Class-Prior {{Estimation}} for {{Learning}} from {{Positive}} and {{Unlabeled Data}}},
  author = {family=Plessis, given=Marthinus C., prefix=du, useprefix=false and Niu, Gang and Sugiyama, Masashi},
  date = {2017-04},
  year = {2017},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {106},
  number = {4},
  eprint = {1611.01586},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {463--492},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-016-5604-6}
}
%  url = {http://arxiv.org/abs/1611.01586},
  urldate = {2023-12-31},
  abstract = {We consider the problem of estimating the class prior in an unlabeled dataset. Under the assumption that an additional labeled dataset is available, the class prior can be estimated by fitting a mixture of class-wise data distributions to the unlabeled data distribution. However, in practice, such an additional labeled dataset is often not available. In this paper, we show that, with additional samples coming only from the positive class, the class prior of the unlabeled dataset can be estimated correctly. Our key idea is to use properly penalized divergences for model fitting to cancel the error caused by the absence of negative samples. We further show that the use of the penalized \$L\_1\$-distance gives a computationally efficient algorithm with an analytic solution. The consistency, stability, and estimation error are theoretically analyzed. Finally, we experimentally demonstrate the usefulness of the proposed method.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {36 citations (Crossref) [2023-12-31]},
  file = {/home/pd/Zotero/storage/6E7HYWKZ/Plessis et al. - 2017 - Class-prior Estimation for Learning from Positive .pdf;/home/pd/Zotero/storage/662Q4REV/1611.html}
}

@incollection{sechidis2011stratification,
  title = {On the {{Stratification}} of {{Multi-label Data}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  journaltitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  editor = {Gunopulos, Dimitrios and Hofmann, Thomas and Malerba, Donato and Vazirgiannis, Michalis},
  date = {2011},
  year = {2011},
  volume = {6913},
  pages = {145--158},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
}
%  doi = {10.1007/978-3-642-23808-6_10},
  url = {http://link.springer.com/10.1007/978-3-642-23808-6_10},
  urldate = {2024-01-28},
  abstract = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and produces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sampling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
  isbn = {978-3-642-23807-9 978-3-642-23808-6},
  langid = {english},
  file = {/home/pd/Zotero/storage/EKDGYBFA/Sechidis et al. - 2011 - On the Stratification of Multi-label Data.pdf}
}

@inproceedings{szymanski2017network,
  title = {A {{Network Perspective}} on {{Stratification}} of {{Multi-Label Data}}},
  booktitle = {Proceedings of the {{First International Workshop}} on {{Learning}} with {{Imbalanced Domains}}: {{Theory}} and {{Applications}}},
  journaltitle = {Proceedings of the {{First International Workshop}} on {{Learning}} with {{Imbalanced Domains}}: {{Theory}} and {{Applications}}},
  author = {Szyma\'nski, Piotr and Kajdanowicz, Tomasz},
  date = {2017-10-11},
  year = {2017},
  pages = {22--35},
  publisher = {{PMLR}},
  issn = {2640-3498} }
 % url = {https://proceedings.mlr.press/v74/szyma%C5%84ski17a.html},
  urldate = {2024-01-28},
  abstract = {We present a new approach to stratifying multi-label data for classification purposes based on the iterative stratification approach proposed by Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the iterative approach to take into account second-order relationships between labels. Obtained results are evaluated using statistical properties of obtained strata as presented by Sechidis. We also propose new statistical measures relevant to second-order quality: label pairs distribution, the percentage of label pairs without positive evidence in folds and label pair - fold pairs that have no positive evidence for the label pair. We verify the impact of new methods on classification performance of Binary Relevance, Label Powerset and a fast greedy community detection based label space partitioning classifier. The proposed approach lowers the variance of classification quality, improves label pair oriented measures and example distribution while maintaining a competitive quality in label-oriented measures. We also witness an increase in stability of network characteristics.},
  eventtitle = {First {{International Workshop}} on {{Learning}} with {{Imbalanced Domains}}: {{Theory}} and {{Applications}}},
  langid = {english},
  file = {/home/pd/Zotero/storage/VFSF5A5C/Szymański and Kajdanowicz - 2017 - A Network Perspective on Stratification of Multi-L.pdf}
}

@article{wang2020learning,
  title = {Learning from {{Weak-Label Data}}: {{A Deep Forest Expedition}}},
  shorttitle = {Learning from {{Weak-Label Data}}},
  author = {Wang, Qian-Wei and Yang, Liang and Li, Yu-Feng},
  year = {2020},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {6251--6258},
  issn = {2374-3468} }
%  doi = {10.1609/aaai.v34i04.6092},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/6092},
  urldate = {2023-12-18},
  abstract = {Weak-label learning deals with the problem where each training example is associated with multiple ground-truth labels simultaneously but only partially provided. This circumstance is frequently encountered when the number of classes is very large or when there exists a large ambiguity between class labels, and significantly influences the performance of multi-label learning. In this paper, we propose LCForest, which is the first tree ensemble based deep learning method for weak-label learning. Rather than formulating the problem as a regularized framework, we employ the recently proposed cascade forest structure, which processes information layer-by-layer, and endow it with the ability of exploiting from weak-label data by a concise and highly efficient label complement structure. Specifically, in each layer, the label vector of each instance from testing-fold is modified with the predictions of random forests trained with the corresponding training-fold. Since the ground-truth label matrix is inaccessible, we can not estimate the performance via cross-validation directly. In order to control the growth of cascade forest, we adopt label frequency estimation and the complement flag mechanism. Experiments show that the proposed LCForest method compares favorably against the existing state-of-the-art multi-label and weak-label learning methods.},
  issue = {04},
  langid = {english},
  annotation = {5 citations (Crossref) [2023-12-18]},
  file = {/home/pd/Zotero/storage/Z34QIXNR/Wang et al. - 2020 - Learning from Weak-Label Data A Deep Forest Exped.pdf}
}

@online{zhao2022boosting,
  title = {A {{Boosting Algorithm}} for {{Positive-Unlabeled Learning}}},
  author = {Zhao, Yawen and Zhang, Mingzhe and Zhang, Chenhao and Chen, Weitong and Ye, Nan and Xu, Miao},
  date = {2022-12-07},
  year = {2022},
  eprint = {2205.09485},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.09485},
}
 % url = {http://arxiv.org/abs/2205.09485},
  urldate = {2023-12-31},
  abstract = {Positive-unlabeled (PU) learning deals with binary classification problems when only positive (P) and unlabeled (U) data are available. Many recent PU methods are based on neural networks, but little has been done to develop boosting algorithms for PU learning, despite boosting algorithms' strong performance on many fully supervised classification problems. In this paper, we propose a novel boosting algorithm, AdaPU, for PU learning. Similarly to AdaBoost, AdaPU aims to optimize an empirical exponential loss, but the loss is based on the PU data, rather than on positive-negative (PN) data. As in AdaBoost, we learn a weighted combination of weak classifiers by learning one weak classifier and its weight at a time. However, AdaPU requires a very different algorithm for learning the weak classifiers and determining their weights. This is because AdaPU learns a weak classifier and its weight using a weighted positive-negative (PN) dataset with some negative data weights \$-\$ the dataset is derived from the original PU data, and the data weights are determined by the current weighted classifier combination, but some data weights are negative. Our experiments showed that AdaPU outperforms neural networks on several benchmark PU datasets, including a large-scale challenging cyber security dataset.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/pd/Zotero/storage/4KZW89TQ/Zhao et al. - 2022 - A Boosting Algorithm for Positive-Unlabeled Learni.pdf;/home/pd/Zotero/storage/GJEKD24T/2205.html}
}

@article{zhou2019deep,
  title = {Deep Forest},
  author = {Zhou, Zhi-Hua and Feng, Ji},
  date = {2019-01-01},
  year = {2019},
  journaltitle = {National Science Review},
  journal = {National Science Review},
  shortjournal = {National Science Review},
  volume = {6},
  number = {1},
  pages = {74--86},
  issn = {2095-5138},
  doi = {10.1093/nsr/nwy108}
}
  %url = {https://doi.org/10.1093/nsr/nwy108},
  urldate = {2023-12-18},
  abstract = {Current deep-learning models are mostly built upon neural networks, i.e. multiple layers of parameterized differentiable non-linear modules that can be trained by backpropagation. In this paper, we explore the possibility of building deep models based on non-differentiable modules such as decision trees. After a discussion about the mystery behind deep neural networks, particularly by contrasting them with shallow neural networks and traditional machine-learning techniques such as decision trees and boosting machines, we conjecture that the success of deep neural networks owes much to three characteristics, i.e. layer-by-layer processing, in-model feature transformation and sufficient model complexity. On one hand, our conjecture may offer inspiration for theoretical understanding of deep learning; on the other hand, to verify the conjecture, we propose an approach that generates deep forest holding these characteristics. This is a decision-tree ensemble approach, with fewer hyper-parameters than deep neural networks, and its model complexity can be automatically determined in a data-dependent way. Experiments show that its performance is quite robust to hyper-parameter settings, such that in most cases, even across different data from different domains, it is able to achieve excellent performance by using the same default setting. This study opens the door to deep learning based on non-differentiable modules without gradient-based adjustment, and exhibits the possibility of constructing deep models without backpropagation.},
  annotation = {323 citations (Crossref) [2023-12-18]},
  file = {/home/pd/Zotero/storage/FLGZNTY8/Zhou and Feng - 2019 - Deep forest.pdf;/home/pd/Zotero/storage/KUIT2NKQ/5123737.html}
}
@article{grinsztajn2022tree,
  title={Why do tree-based models still outperform deep learning on typical tabular data?},
  author={Grinsztajn, L{\'e}o and Oyallon, Edouard and Varoquaux, Ga{\"e}l},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={507--520},
  year={2022}
}

@article{read2021classifier,
  title={Classifier chains: A review and perspectives},
  author={Read, Jesse and Pfahringer, Bernhard and Holmes, Geoffrey and Frank, Eibe},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={683--718},
  year={2021}
}
@article{xu2019survey,
  title={Survey on multi-output learning},
  author={Xu, Donna and Shi, Yaxin and Tsang, Ivor W and Ong, Yew-Soon and Gong, Chen and Shen, Xiaobo},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={7},
  pages={2409--2429},
  year={2019},
  publisher={IEEE}
}
@article{waegeman2019multi,
  title={Multi-target prediction: a unifying view on problems and methods},
  author={Waegeman, Willem and Dembczy{\'n}ski, Krzysztof and H{\"u}llermeier, Eyke},
  journal={Data Mining and Knowledge Discovery},
  volume={33},
  pages={293--324},
  year={2019},
  publisher={Springer}
}
@article{zhou2018brief,
  title={A brief introduction to weakly supervised learning},
  author={Zhou, Zhi-Hua},
  journal={National science review},
  volume={5},
  number={1},
  pages={44--53},
  year={2018},
  publisher={Oxford University Press}
}
@article{yang2019multi,
  title={Multi-label learning with deep forest},
  author={Yang, Liang and Wu, Xi-Zhu and Jiang, Yuan and Zhou, Zhi-Hua},
  journal={arXiv preprint arXiv:1911.06557},
  year={2019}
}
@article{guo2018bcdforest,
  title={BCDForest: a boosting cascade deep forest model towards the classification of cancer subtypes based on gene expression data},
  author={Guo, Yang and Liu, Shuhui and Li, Zhanhuai and Shang, Xuequn},
  journal={BMC bioinformatics},
  volume={19},
  number={5},
  pages={1--13},
  year={2018},
  publisher={BioMed Central}
}
@article{utkin2018siamese,
  title={A siamese deep forest},
  author={Utkin, Lev V and Ryabinin, Mikhail A},
  journal={Knowledge-Based Systems},
  volume={139},
  pages={13--22},
  year={2018},
  publisher={Elsevier}
}
@article{zhou2019hashing,
  title={Deep forest hashing for image retrieval},
  author={Zhou, Meng and Zeng, Xianhua and Chen, Aozhu},
  journal={Pattern Recognition},
  volume={95},
  pages={114--127},
  year={2019},
  publisher={Elsevier}
}
@article{su2019deep,
  title={Deep-Resp-Forest: a deep forest model to predict anti-cancer drug response},
  author={Su, Ran and Liu, Xinyi and Wei, Leyi and Zou, Quan},
  journal={Methods},
  volume={166},
  pages={91--102},
  year={2019},
  publisher={Elsevier}
}
@article{ma2020cost,
  title={Cost-sensitive deep forest for price prediction},
  author={Ma, Chao and Liu, Zhenbing and Cao, Zhiguang and Song, Wen and Zhang, Jie and Zeng, Weiliang},
  journal={Pattern Recognition},
  volume={107},
  pages={107499},
  year={2020},
  publisher={Elsevier}
}
@article{zhang2007ml,
  title={ML-KNN: A lazy learning approach to multi-label learning},
  author={Zhang, Min-Ling and Zhou, Zhi-Hua},
  journal={Pattern recognition},
  volume={40},
  number={7},
  pages={2038--2048},
  year={2007},
  publisher={Elsevier}
}
@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{sun2010multi,
  title={Multi-label learning with weak label},
  author={Sun, Yu-Yin and Zhang, Yin and Zhou, Zhi-Hua},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  journaltitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={24},
  number={1},
  pages={593--598},
  year={2010}
}
@article{xu2013speedup,
  title={Speedup matrix completion with side information: Application to multi-label learning},
  author={Xu, Miao and Jin, Rong and Zhou, Zhi-Hua},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@inproceedings{tsoumakas2007random,
  title={Random k-labelsets: An ensemble method for multilabel classification},
  author={Tsoumakas, Grigorios and Vlahavas, Ioannis},
  booktitle={European conference on machine learning},
  journaltitle={European conference on machine learning},
  pages={406--417},
  year={2007},
  organization={Springer}
}
@inproceedings{wang2020dual,
  title={Dual relation semi-supervised multi-label learning},
  author={Wang, Lichen and Liu, Yunyu and Qin, Can and Sun, Gan and Fu, Yun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  journaltitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={6227--6234},
  year={2020}
}
@inproceedings{xie2018partial,
  title={Partial multi-label learning},
  author={Xie, Ming-Kun and Huang, Sheng-Jun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  journaltitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{nakano2019machine,
  title={Machine learning for discovering missing or wrong protein function annotations: A comparison using updated benchmark datasets},
  author={Nakano, Felipe Kenji and Lietaert, Mathias and Vens, Celine},
  journal={BMC bioinformatics},
  volume={20},
  pages={1--32},
  year={2019},
  publisher={Springer}
}
@inproceedings{zhan2017inductive,
  title={Inductive semi-supervised multi-label learning with co-training},
  author={Zhan, Wang and Zhang, Min-Ling},
  booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  journaltitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1305--1314},
  year={2017}
}
